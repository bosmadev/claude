# Future Workflow Enhancements

**Created:** 2026-01-26
**Purpose:** Deferred items for future consideration when time/resources permit

---

## Table of Contents

1. [Tool Input Validation](#1-tool-input-validation)
2. [Observability Stack](#2-observability-stack)
3. [Skill Discovery Enhancement](#3-skill-discovery-enhancement)
4. [Skill Versioning](#4-skill-versioning)
5. [Metrics Collection & Monitoring](#5-metrics-collection--monitoring)
6. [Log Aggregation](#6-log-aggregation)
7. [Secrets Management](#7-secrets-management)
8. [Input Sanitization](#8-input-sanitization)
9. [ML-Based Stuck Detection](#9-ml-based-stuck-detection)
10. [MCP Gateway Pattern](#10-mcp-gateway-pattern)

---

## 1. Tool Input Validation

**Current State:** Trust MCP tool inputs
**Goal:** Schema validation at gateway/hook level

### Option A: Zod Schema Enforcement

| Aspect | Details |
|--------|---------|
| **Implementation** | Define Zod schemas for all MCP tool inputs, validate before execution |
| **Pros** | - Type safety<br>- Clear error messages<br>- Documentation as code |
| **Cons** | - Schema maintenance<br>- Validation overhead |
| **Use Case** | All MCP tools |

```typescript
const scrapeSchema = z.object({
  url: z.string().url(),
  options: z.object({
    stealth: z.boolean().default(true),
    timeout: z.number().max(60000).default(30000)
  }).optional()
});
```

### Option B: Runtime Type Guards

| Aspect | Details |
|--------|---------|
| **Implementation** | TypeScript guards with runtime assertions |
| **Pros** | - Lighter weight<br>- No extra dependency<br>- Compile-time hints |
| **Cons** | - Less comprehensive<br>- Manual maintenance |
| **Use Case** | Small tool sets |

**Recommendation:** Option A for production, Option B for prototyping

---

## 2. Observability Stack

**Current State:** File-based logging
**Goal:** Structured observability with tracing

### Option A: OpenTelemetry Integration

| Aspect | Details |
|--------|---------|
| **Implementation** | Instrument hooks with OTel traces, export to Jaeger/Grafana |
| **Pros** | - Distributed tracing<br>- Industry standard<br>- Rich visualization |
| **Cons** | - Setup complexity<br>- Storage requirements |
| **Use Case** | Multi-agent debugging, performance analysis |

```python
from opentelemetry import trace
tracer = trace.get_tracer("ralph")
with tracer.start_as_current_span("agent-task"):
    # Task execution
```

### Option B: JSON Log Aggregation

| Aspect | Details |
|--------|---------|
| **Implementation** | Structured JSON logs to `.claude/logs/`, aggregate with jq |
| **Pros** | - Simple<br>- No external dependencies<br>- Grep-friendly |
| **Cons** | - Limited visualization<br>- Manual analysis |
| **Use Case** | Local development, quick debugging |

**Recommendation:** Option B for local, migrate to Option A for production/team use

---

## 3. Skill Discovery Enhancement

**Current State:** List all skills in flat directory
**Goal:** Search and filter skills by context

### Option A: Tag-Based Discovery

| Aspect | Details |
|--------|---------|
| **Implementation** | Add `tags: [browser, scraping, stealth]` to skill frontmatter |
| **Pros** | - Semantic search<br>- Multiple categories<br>- Easy filtering |
| **Cons** | - Tagging discipline required |
| **Use Case** | Large skill libraries |

Example frontmatter:
```yaml
---
name: scraper
tags: [browser, scraping, stealth, web]
description: Web scraping with anti-detection
---
```

### Option B: Full-Text Search

| Aspect | Details |
|--------|---------|
| **Implementation** | Index skill names, descriptions, and content for search |
| **Pros** | - Find anything<br>- No manual tagging<br>- Covers documentation |
| **Cons** | - Less precise<br>- Index maintenance |
| **Use Case** | Organic skill discovery |

**Recommendation:** Both - Tags for organization, full-text for discovery

**Note:** Claude Code issue #18192 tracks nested skills discovery. Current workaround is sub-files within skill folders (e.g., `commit/SKILL.md`, `commit/pending-commit-template.md`).

---

## 4. Skill Versioning

**Current State:** No version tracking
**Goal:** Semantic versioning for skills

### Option A: Git-Based Versioning

| Aspect | Details |
|--------|---------|
| **Implementation** | Add `version: 1.2.0` to frontmatter, track in git tags |
| **Pros** | - Leverages existing git<br>- Easy rollback<br>- Diff support |
| **Cons** | - Manual version bumps |
| **Use Case** | Team skill sharing |

### Option B: Auto-Version from Changelog

| Aspect | Details |
|--------|---------|
| **Implementation** | Parse `## Changelog` section in skill, auto-increment version |
| **Pros** | - Automatic<br>- Encourages documentation |
| **Cons** | - Requires changelog discipline |
| **Use Case** | Solo developers |

**Recommendation:** Option A for teams, Option B for personal use

---

## 5. Metrics Collection & Monitoring

**Current State:** No metrics
**Goal:** Comprehensive metrics for Ralph operations

### Option A: Prometheus Metrics Endpoint

| Aspect | Details |
|--------|---------|
| **Implementation** | Expose `/metrics` with Ralph stats, scrape with Prometheus |
| **Pros** | - Standard format<br>- Grafana dashboards<br>- Alerting |
| **Cons** | - Infrastructure overhead |
| **Use Case** | Team/production environments |

Proposed metrics:
- `ralph_agents_total` - Total agents spawned
- `ralph_tasks_completed` - Tasks by status
- `ralph_iteration_duration_seconds` - Histogram
- `ralph_stuck_events_total` - Stuck occurrences

### Option B: Status Line Metrics

| Aspect | Details |
|--------|---------|
| **Implementation** | Enhance `statusline.sh` to show real-time stats |
| **Pros** | - No infrastructure<br>- Immediate visibility<br>- Already integrated |
| **Cons** | - No historical data |
| **Use Case** | Local development |

**Recommendation:** Option B for local, Option A for team dashboards

---

## 6. Log Aggregation

**Current State:** Separate log files per component
**Goal:** Unified log stream

### Option A: Structured JSON Logs

| Aspect | Details |
|--------|---------|
| **Implementation** | All components emit JSON to `.claude/logs/unified.jsonl` |
| **Pros** | - Machine parseable<br>- Easy filtering<br>- Timestamp correlation |
| **Cons** | - Larger files<br>- Requires tooling |
| **Use Case** | Post-mortem analysis |

```json
{"ts":"2026-01-26T12:00:00Z","level":"info","component":"ralph","event":"agent_spawned","agent_id":1}
```

### Option B: Log Streaming to Terminal

| Aspect | Details |
|--------|---------|
| **Implementation** | `tail -f` style streaming with color coding |
| **Pros** | - Real-time<br>- Human readable<br>- No storage |
| **Cons** | - Ephemeral<br>- No search |
| **Use Case** | Active debugging |

**Recommendation:** Both - JSON for persistence, streaming for active monitoring

---

## 7. Secrets Management

**Current State:** Environment variables
**Goal:** Secure secrets handling

### Option A: 1Password/Bitwarden Integration

| Aspect | Details |
|--------|---------|
| **Implementation** | Fetch secrets at runtime via CLI (`op read`, `bw get`) |
| **Pros** | - No secrets in env<br>- Rotation support<br>- Audit trail |
| **Cons** | - External dependency<br>- Network latency |
| **Use Case** | Production, team environments |

```bash
export GITHUB_TOKEN=$(op read "op://vault/github/token")
```

### Option B: Encrypted .env

| Aspect | Details |
|--------|---------|
| **Implementation** | Use `age` or `sops` to encrypt `.env.local` |
| **Pros** | - No external service<br>- Git-storable (encrypted)<br>- Offline capable |
| **Cons** | - Key management<br>- Manual rotation |
| **Use Case** | Single developer, offline environments |

**Recommendation:** Option A for teams, Option B for solo developers

---

## 8. Input Sanitization

**Current State:** Trust user input
**Goal:** Comprehensive sanitization

### Option A: DOMPurify for HTML

| Aspect | Details |
|--------|---------|
| **Implementation** | Sanitize all scraped HTML before processing |
| **Pros** | - XSS prevention<br>- Malicious script removal<br>- Industry standard |
| **Cons** | - Processing overhead |
| **Use Case** | All web scraping |

### Option B: Schema Validation

| Aspect | Details |
|--------|---------|
| **Implementation** | Validate all inputs against Zod schemas |
| **Pros** | - Type safety<br>- Early rejection<br>- Clear errors |
| **Cons** | - Schema maintenance |
| **Use Case** | API inputs |

**Recommendation:** Both - DOMPurify for HTML, Zod for structured data

---

## 9. ML-Based Stuck Detection

**Current State:** Heuristic rules (3 identical errors = stuck)
**Goal:** Intelligent pattern detection

### Why Deferred

Training and maintaining a local ML model in Kali requires:
- Training data collection from stuck patterns
- Model selection (anomaly detection vs classification)
- Integration with Ralph's real-time loop
- Ongoing model maintenance

### Current Heuristics (Keep Using)

```json
{
  "same_error_threshold": 3,
  "no_progress_iterations": 5,
  "build_fail_threshold": 3,
  "circular_dependency_check": true
}
```

### Future Option: Pattern-Based ML Detection

| Aspect | Details |
|--------|---------|
| **Implementation** | Train model on stuck patterns (loops, repeated errors) |
| **Pros** | - Catches subtle patterns<br>- Learns from history<br>- Fewer false positives |
| **Cons** | - Training data needed<br>- Model maintenance |
| **Use Case** | Mature deployments with history |

**Recommendation:** Stick with heuristic rules, evolve to ML only when:
1. Sufficient training data exists (100+ stuck instances)
2. False positive rate from heuristics becomes problematic
3. Resources available for model maintenance

### Implementation Steps (When Ready)

1. **Data Collection Phase** (2-4 weeks)
   ```python
   # Add to guards.py ralph-agent-tracker
   def log_stuck_pattern(context: dict):
       """Log stuck patterns for future ML training."""
       with open("/.claude/ml-training/stuck-patterns.jsonl", "a") as f:
           f.write(json.dumps({
               "timestamp": datetime.utcnow().isoformat(),
               "error_sequence": context.get("recent_errors", []),
               "iteration_count": context.get("iterations", 0),
               "task_type": context.get("task_type"),
               "was_stuck": True,  # Label for training
               "resolution": context.get("resolution")  # How it was resolved
           }) + "\n")
   ```

2. **Model Selection**
   - **Isolation Forest** - Anomaly detection, no labels needed
   - **Random Forest** - Classification if labeled data available
   - **LSTM** - Sequence patterns if error sequences are important

3. **Integration Hook**
   ```python
   # In guards.py
   def check_stuck_ml(context: dict) -> bool:
       """ML-based stuck detection (future)."""
       if not ML_MODEL_PATH.exists():
           return check_stuck_heuristic(context)  # Fallback

       model = load_model(ML_MODEL_PATH)
       features = extract_features(context)
       prediction = model.predict([features])[0]
       confidence = model.predict_proba([features])[0].max()

       if confidence > 0.8 and prediction == "stuck":
           return True
       return check_stuck_heuristic(context)  # Hybrid approach
   ```

4. **Training Pipeline**
   ```bash
   # When 100+ samples collected
   python3 /usr/share/claude/scripts/train-stuck-model.py \
     --input /.claude/ml-training/stuck-patterns.jsonl \
     --output /usr/share/claude/models/stuck-detector.pkl \
     --model isolation_forest
   ```

---

## 10. MCP Gateway Pattern

**Current State:** Direct MCP server connections via hooks
**Goal:** Centralized gateway for enterprise deployments

### Why Deferred

For single-user setup, current hooks provide sufficient control:
- `PreToolUse` → Auth, rate limiting
- `PostToolUse` → Audit logging
- `guards.py` → Policy enforcement

### When to Consider

- Team deployments (>3 users)
- Compliance requirements (SOC2, HIPAA)
- Need for centralized metrics/dashboards

### Option A: Full Gateway Proxy

| Aspect | Details |
|--------|---------|
| **Implementation** | Deploy gateway service that proxies all MCP calls |
| **Pros** | - Centralized auth<br>- Rate limiting<br>- Audit logging<br>- Policy enforcement |
| **Cons** | - Additional latency<br>- Single point of failure<br>- Operational overhead |
| **Use Case** | Enterprise deployments, compliance requirements |

Architecture:
```
Agent -> MCP Gateway -> serena/context7/playwriter
             |
             +-> Auth, Rate Limit, Audit Log
```

### Option B: Sidecar Pattern

| Aspect | Details |
|--------|---------|
| **Implementation** | Run lightweight proxy alongside each Claude instance |
| **Pros** | - Distributed<br>- No single point of failure<br>- Lower latency |
| **Cons** | - Per-instance overhead<br>- Harder to aggregate logs |
| **Use Case** | Developer workstations, CI/CD |

### Implementation Steps (When Ready)

1. **Gateway Service** (Node.js or Python)
   ```python
   # /usr/share/claude/services/mcp-gateway/server.py
   from fastapi import FastAPI, Depends
   from fastapi.middleware.cors import CORSMiddleware

   app = FastAPI()

   @app.middleware("http")
   async def audit_log(request, call_next):
       """Log all MCP calls."""
       start = time.time()
       response = await call_next(request)
       duration = time.time() - start
       log_mcp_call(request, response, duration)
       return response

   @app.post("/mcp/{server}/{tool}")
   async def proxy_mcp(server: str, tool: str, payload: dict):
       """Proxy MCP calls with auth and rate limiting."""
       check_rate_limit(request.client.host)
       check_auth(request.headers.get("Authorization"))
       return await forward_to_mcp(server, tool, payload)
   ```

2. **Configuration**
   ```json
   // settings.json addition
   {
     "mcpGateway": {
       "enabled": false,
       "url": "http://localhost:8080",
       "authToken": "${MCP_GATEWAY_TOKEN}"
     }
   }
   ```

3. **Systemd Service** (when ready)
   ```ini
   # /etc/systemd/user/mcp-gateway.service
   [Unit]
   Description=MCP Gateway Service
   After=network.target

   [Service]
   ExecStart=/usr/bin/python3 /usr/share/claude/services/mcp-gateway/server.py
   Restart=always

   [Install]
   WantedBy=default.target
   ```

**Recommendation:** Skip for now. Revisit when team size > 3 or compliance needed.

---

## Priority Matrix

| Item | Complexity | Value | When to Consider |
|------|------------|-------|------------------|
| Tool Input Validation | Medium | Medium | When adding new MCP tools |
| Observability Stack | High | High | When debugging multi-agent issues |
| Skill Discovery Tags | Low | Medium | When skill count > 20 |
| Skill Versioning | Low | Low | When sharing skills externally |
| Prometheus Metrics | High | Medium | When running team instances |
| Log Aggregation | Medium | Medium | When post-mortem analysis needed |
| Secrets Management | Medium | High | When adding sensitive integrations |
| Input Sanitization | Medium | High | When handling untrusted web content |
| ML Stuck Detection | High | Medium | When heuristics fail frequently |
| MCP Gateway | High | Low | When team size > 3 or compliance needed |

---

## References

- [Zod Schema Library](https://zod.dev/)
- [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)
- [DOMPurify](https://github.com/cure53/DOMPurify)
- [SOPS - Secrets Operations](https://github.com/getsops/sops)
- [Age Encryption](https://github.com/FiloSottile/age)
